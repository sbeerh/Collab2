{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7B8CP5MAJF0"
      },
      "source": [
        "# Train NanoDet with custom dataset\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SonySemiconductorSolutions/aitrios-rpi-tutorials-ai-model-training/blob/main/notebooks/nanodet-ppe/custom_nanodet.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Training NanoDet model to detect Personal Protection Equipment (PPE) using open source dataset.\n",
        "\n",
        "Observe, that this tutorial requires GPU when used with Colab. Enable GPU:\n",
        "\n",
        "* Navigate to Edit→Notebook Settings\n",
        "* Select T4 GPU from Hardware Accelerator\n",
        "\n",
        "Nanodet training based on https://github.com/RangiLyu/nanodet/tree/main\n",
        "\n",
        "Tutorial includes:\n",
        "- Dataset setup\n",
        "- Nanodet model setup\n",
        "- Training\n",
        "- Quantization using [Model Compression Toolkit - MCT](https://github.com/sony/model_optimization)\n",
        "- COCO evaluation\n",
        "- Visualization\n",
        "- Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6eBwapPAJF4",
        "outputId": "23f6e039-8d31-4d9e-aeb4-55e2b324d3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch~=2.5.0\n",
            "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.15.1 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.15.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Expected error message:\n",
        "\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
        "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.5 which is incompatible.\n",
        "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.5 which is incompatible.\n",
        "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.0 which is incompatible.\n",
        "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.1 which is incompatible.\n",
        "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.3.2 which is incompatible.\n",
        "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
        "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\n",
        "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
        "\"\"\"\n",
        "!pip install --no-cache-dir torch~=2.5.0 torchvision tensorflow==2.15.1 pycocotools imx500-converter[tf] edge-mdt-tpc \"numpy<2\" opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7rAdPJ9AJF6",
        "outputId": "adfb0eae-20c5-413a-dffc-952671d4877c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found Java version: 11.0.28\n",
            "Java 17 is not installed. Installing correct version...\n",
            "Java installed\n"
          ]
        }
      ],
      "source": [
        "# Converter requires java\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "\n",
        "def install_java(package: str = 'openjdk-17-jdk', version: int = 17) -> bool:\n",
        "    try:\n",
        "        result = subprocess.run(['java', '--version'], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        version_output = result.stdout.splitlines()[0]\n",
        "        match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', version_output)  # Match version in form major.minor.patch\n",
        "        print(f\"Found Java version: {match.group(0)}\")\n",
        "        if match:\n",
        "            major_version = int(match.group(1))\n",
        "            if major_version == version:\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Java {version} is not installed. Installing correct version...\")\n",
        "    except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
        "        print(f\"Java not installed. Installing...\")\n",
        "\n",
        "    try:\n",
        "        is_root = os.geteuid() == 0\n",
        "        prefix = [] if is_root else ['sudo']\n",
        "        with open(os.devnull, 'w') as devnull:\n",
        "            subprocess.run(prefix + ['apt', 'install', '-y', package], check=True, stdout=devnull, stderr=devnull)\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Installation error: {e}\")\n",
        "        return False\n",
        "\n",
        "if install_java():\n",
        "    print(f'Java installed')\n",
        "else:\n",
        "    print(f'Java missing and installation failed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H80YhNiPAJF6"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4drY_lAAJF7",
        "outputId": "494d98f5-4f51-4a7d-986e-51d4d2cc94ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanodet'...\n",
            "remote: Enumerating objects: 2722, done.\u001b[K\n",
            "remote: Total 2722 (delta 0), reused 0 (delta 0), pack-reused 2722 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2722/2722), 5.29 MiB | 4.63 MiB/s, done.\n",
            "Resolving deltas: 100% (1602/1602), done.\n",
            "Branch 'pytorch2.0' set up to track remote branch 'pytorch2.0' from 'origin'.\n",
            "Switched to a new branch 'pytorch2.0'\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m166.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Known errors:\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
        "torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
        "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
        "\"\"\"\n",
        "NANODET_COMMIT = 'pytorch2.0'\n",
        "!rm -rf nanodet\n",
        "!git clone https://github.com/RangiLyu/nanodet.git\n",
        "!touch nanodet/nanodet/model/__init__.py\n",
        "!cd nanodet && git checkout {NANODET_COMMIT} && pip install -q --no-cache-dir -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn7so20WAJF7",
        "outputId": "c70d0eaa-36d6-4b90-b68e-f5f5cbc3faff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is cuda available: True\n"
          ]
        }
      ],
      "source": [
        "# Perform initial checks in order to continue\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "#assert '2.15' in tf.__version__, print(tf.__version__)\n",
        "#assert '2.5.' in torch.__version__, print(torch.__version__)\n",
        "#assert '1' in np.__version__[0], print(np.__version__)\n",
        "\n",
        "print(f'Is cuda available: {torch.cuda.is_available()}')\n",
        "assert torch.cuda.is_available(), \"GPU is required, for Colab see for example, https://colab.research.google.com/notebooks/gpu.ipynb\" # training requires 254M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1UIdxRiAJF7"
      },
      "source": [
        "# Dataset\n",
        "- go to https://universe.roboflow.com/ai-camp-safety-equipment-detection/ppe-detection-using-cv/dataset/3 and click `\"Download Dataset\"`\n",
        "- choose format `\"COCO\"` and `\"show download code\"` and `\"continue\"`\n",
        "- choose `\"Terminal\"` and copy the command `\"curl...\"` and paste the command in the cell below.\n",
        "- add `\"!\"` in the beginning of the command and replace `\"\\&gt;\"` with `\">\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9r7jsOdAJF8",
        "outputId": "c0836d17-3c81-48e9-f264-07d0325b62f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  7073    0  7073    0     0  72329      0 --:--:-- --:--:-- --:--:-- 72917\n",
            "Archive:  roboflow.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of roboflow.zip or\n",
            "        roboflow.zip.zip, and cannot find roboflow.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "# Add below your download code from Roboflow, it should look like the following, with your unique roboflow dataset url:\n",
        "# Example (with \"!\" added in the beginning of the command and replaced \"&gt;\" with \">\". Also added \"-q\" for less output):\n",
        "# !curl -L \"https://universe.roboflow.com/ds/<unique-dataset-url>\" > roboflow.zip; unzip -q roboflow.zip; rm roboflow.zip\n",
        "!curl -L \"https://universe.roboflow.com/ds/J5viLPA0jB?key=O0CBXu2KVs\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dd7e733",
        "outputId": "bfd8cb09-dba6-4bf5-9987-a5593decd66b"
      },
      "source": [
        "# Try using wget to download the data\n",
        "!wget -O roboflow.zip \"https://universe.roboflow.com/ds/J5viLPA0jB?key=O0CBXu2KVs\" && unzip -q roboflow.zip && rm roboflow.zip"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-24 11:31:35--  https://universe.roboflow.com/ds/J5viLPA0jB?key=O0CBXu2KVs\n",
            "Resolving universe.roboflow.com (universe.roboflow.com)... 172.66.41.4, 172.66.42.252, 2606:4700:3108::ac42:2904, ...\n",
            "Connecting to universe.roboflow.com (universe.roboflow.com)|172.66.41.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2025-09-24 11:31:35 ERROR 403: Forbidden.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "msqE1M56AJF8",
        "outputId": "6587b12f-e317-4a9a-f9ee-03605eec86ff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-36401162.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDATASET_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dataset/PPE_Detection_Using_CV.v3i.coco'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATASET_PATH}/train/_annotations.coco.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'train/_annotations.coco.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'valid/_annotations.coco.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'test/_annotations.coco.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Move test/train/valid to dataset folder\n",
        "from pathlib import Path\n",
        "DATASET_PATH = 'dataset/PPE_Detection_Using_CV.v3i.coco'\n",
        "if not Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists():\n",
        "    assert Path(f'train/_annotations.coco.json').exists()\n",
        "    assert Path(f'valid/_annotations.coco.json').exists()\n",
        "    assert Path(f'test/_annotations.coco.json').exists()\n",
        "    !mkdir -p $DATASET_PATH\n",
        "    !mv test train valid *txt $DATASET_PATH/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jljk_qZ2AJF9"
      },
      "outputs": [],
      "source": [
        "assert Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists()\n",
        "assert Path(f'{DATASET_PATH}/valid/_annotations.coco.json').exists()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K32W0SoZAJF9"
      },
      "source": [
        "# Training config file: nanodet-plus-m-1.5x_416-ppe.yml\n",
        "The following block of code creates the NanoDet training config file which\n",
        "is based on nanodet/config/nanodet-plus-m-1.5x_416.yml.\n",
        "Updated for the custom PPE dataset\n",
        "Change number of `total_epochs` for better performance.\n",
        "\n",
        "If training on more than 1 GPU, then set `gpu_ids`:\n",
        " * 2 gpu: [0,1]\n",
        " * etc...\n",
        "\n",
        "Increase `total_epochs`, for example 20.\n",
        "\n",
        "Feel free to increase `val_intervals`, for example 10.\n",
        "\n",
        "## Resume training\n",
        "Uncomment `resume` and `load_model` and add path to trained weights, For example: `workspace/nanodet-plus-m-1.5x_416-ppe/model_last.ckpt`\n",
        "\n",
        "For details see NanoDet github repo and [config docs](https://github.com/RangiLyu/nanodet/blob/main/docs/config_file_detail.md). Observe recommendation to adjust `lr` with `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFIZdacqAJF9"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "touch nanodet-plus-m-1.5x_416-ppe.yml\n",
        "cat <<EOF >nanodet-plus-m-1.5x_416-ppe.yml\n",
        "# Comments:\n",
        "# -  based on nanodet/config/nanodet-plus-m-1.5x_416.yml\n",
        "# -  \"device\": settings for colab T4 GPU\n",
        "# -  \"total_epochs\": set to 20 during testing, default 300\n",
        "save_dir: workspace/nanodet-plus-m-1.5x_416-ppe\n",
        "model:\n",
        "  weight_averager:\n",
        "    name: ExpMovingAverager\n",
        "    decay: 0.9998\n",
        "  arch:\n",
        "    name: NanoDetPlus\n",
        "    detach_epoch: 10\n",
        "    backbone:\n",
        "      name: ShuffleNetV2\n",
        "      model_size: 1.5x\n",
        "      out_stages: [2,3,4]\n",
        "      activation: LeakyReLU\n",
        "    fpn:\n",
        "      name: GhostPAN\n",
        "      in_channels: [176, 352, 704]\n",
        "      out_channels: 128\n",
        "      kernel_size: 5\n",
        "      num_extra_level: 1\n",
        "      use_depthwise: True\n",
        "      activation: LeakyReLU\n",
        "    head:\n",
        "      name: NanoDetPlusHead\n",
        "      num_classes: 8\n",
        "      input_channel: 128\n",
        "      feat_channels: 128\n",
        "      stacked_convs: 2\n",
        "      kernel_size: 5\n",
        "      strides: [8, 16, 32, 64]\n",
        "      activation: LeakyReLU\n",
        "      reg_max: 7\n",
        "      norm_cfg:\n",
        "        type: BN\n",
        "      loss:\n",
        "        loss_qfl:\n",
        "          name: QualityFocalLoss\n",
        "          use_sigmoid: True\n",
        "          beta: 2.0\n",
        "          loss_weight: 1.0\n",
        "        loss_dfl:\n",
        "          name: DistributionFocalLoss\n",
        "          loss_weight: 0.25\n",
        "        loss_bbox:\n",
        "          name: GIoULoss\n",
        "          loss_weight: 2.0\n",
        "    # Auxiliary head, only use in training time.\n",
        "    aux_head:\n",
        "      name: SimpleConvHead\n",
        "      num_classes: 8\n",
        "      input_channel: 256\n",
        "      feat_channels: 256\n",
        "      stacked_convs: 4\n",
        "      strides: [8, 16, 32, 64]\n",
        "      activation: LeakyReLU\n",
        "      reg_max: 7\n",
        "data:\n",
        "  train:\n",
        "    name: CocoDataset\n",
        "    img_path: dataset/PPE_Detection_Using_CV.v3i.coco/train\n",
        "    ann_path: dataset/PPE_Detection_Using_CV.v3i.coco/train/_annotations.coco.json\n",
        "    input_size: [416,416] #[w,h]\n",
        "    keep_ratio: False\n",
        "    pipeline:\n",
        "      perspective: 0.0\n",
        "      scale: [0.6, 1.4]\n",
        "      stretch: [[0.8, 1.2], [0.8, 1.2]]\n",
        "      rotation: 0\n",
        "      shear: 0\n",
        "      translate: 0.2\n",
        "      flip: 0.5\n",
        "      brightness: 0.2\n",
        "      contrast: [0.6, 1.4]\n",
        "      saturation: [0.5, 1.2]\n",
        "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n",
        "  val:\n",
        "    name: CocoDataset\n",
        "    img_path: dataset/PPE_Detection_Using_CV.v3i.coco/valid\n",
        "    ann_path: dataset/PPE_Detection_Using_CV.v3i.coco/valid/_annotations.coco.json\n",
        "    input_size: [416,416] #[w,h]\n",
        "    keep_ratio: False\n",
        "    pipeline:\n",
        "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n",
        "device:\n",
        "  gpu_ids: [0]\n",
        "  workers_per_gpu: 2\n",
        "  batchsize_per_gpu: 32\n",
        "  precision: 32 # set to 16 to use AMP training\n",
        "schedule:\n",
        "#  resume:\n",
        "#  load_model:\n",
        "  optimizer:\n",
        "    name: AdamW\n",
        "    lr: 0.001\n",
        "    weight_decay: 0.05\n",
        "  warmup:\n",
        "    name: linear\n",
        "    steps: 500\n",
        "    ratio: 0.0001\n",
        "  total_epochs: 5\n",
        "  lr_schedule:\n",
        "    name: CosineAnnealingLR\n",
        "    T_max: 300\n",
        "    eta_min: 0.00005\n",
        "  val_intervals: 5\n",
        "grad_clip: 35\n",
        "evaluator:\n",
        "  name: CocoDetectionEvaluator\n",
        "  save_key: mAP\n",
        "log:\n",
        "  interval: 10\n",
        "\n",
        "class_names: [\n",
        "  'safety-equipment',\n",
        "  'person',\n",
        "  'goggles',\n",
        "  'helmet',\n",
        "  'no-goggles',\n",
        "  'no-helmet',\n",
        "  'no-vest',\n",
        "  'vest']\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEC7gBowAJF-"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU_nxYG0AJF-"
      },
      "outputs": [],
      "source": [
        "# OBSERVE: update the following assert statement to match your yml file settings.\n",
        "\n",
        "import yaml\n",
        "with open('nanodet-plus-m-1.5x_416-ppe.yml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "gpu_ids = config['device']['gpu_ids']\n",
        "assert isinstance(gpu_ids, list) and gpu_ids, print(f\"gpu_ids: {config['device']['gpu_ids']}\")\n",
        "assert config['schedule']['total_epochs'] == 5 or config['schedule']['total_epochs'] == 20, print(f\"total_epochs: {config['schedule']['total_epochs']}\")\n",
        "assert config['schedule']['val_intervals'] == 5 or config['schedule']['val_intervals'] == 10, print(f\"val_intervals: {config['schedule']['val_intervals']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47u0lF1rAJF-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "assert '2.5.' in torch.__version__, print(torch.__version__)\n",
        "assert Path('nanodet-plus-m-1.5x_416-ppe.yml').exists()\n",
        "!export PYTHONPATH=$PWD/nanodet:$PYTHONPATH && python nanodet/tools/train.py nanodet-plus-m-1.5x_416-ppe.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bUNq79dAJF-"
      },
      "source": [
        "# Remove aux layers that are only used during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEpDYyG-AJF-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,\"./nanodet\")\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "from nanodet.model.arch import build_model\n",
        "from nanodet.util import cfg, load_config, Logger\n",
        "\n",
        "def remove_aux(cfg, model_path, remove_layers=['aux_fpn', 'aux_head'], debug=False):\n",
        "    model = build_model(cfg.model)\n",
        "    ckpt = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "    if len(remove_layers) > 0:\n",
        "        state_dict = copy.deepcopy(ckpt['state_dict'])\n",
        "        for rlayer in remove_layers:\n",
        "            for layer in ckpt['state_dict']:\n",
        "                if rlayer in layer:\n",
        "                    del state_dict[layer]\n",
        "                    if debug:\n",
        "                        print(f'removed layer: {layer}')\n",
        "        del ckpt['state_dict']\n",
        "        ckpt['state_dict'] = copy.deepcopy(state_dict)\n",
        "        del state_dict\n",
        "    return ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKTLGkt5AJF-"
      },
      "outputs": [],
      "source": [
        "config_path = 'nanodet-plus-m-1.5x_416-ppe.yml'\n",
        "model_path = 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth'\n",
        "dst_path = 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best-removed-aux.pth'\n",
        "\n",
        "load_config(cfg, config_path)\n",
        "ckpt = remove_aux(cfg, model_path, ['aux_fpn', 'aux_head'])\n",
        "torch.save(ckpt, dst_path)\n",
        "print(f'Saved to: {dst_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7jCH6sFAJF-"
      },
      "outputs": [],
      "source": [
        "# Compare size w and w/o aux\n",
        "!ls -l workspace/nanodet-plus-m-1.5x_416-ppe/model_best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ounRCKHvAJF_"
      },
      "source": [
        "# Quantization of custom NanoDet model using Model Compression Toolkit\n",
        "Quantization is based on examples from [Model Compression Toolkit Keras Tutorials](https://github.com/sony/model_optimization/blob/v2.0.0/tutorials/notebooks/keras/ptq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhjwisSTAJF_"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEU1m0p_AJF_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import importlib\n",
        "\n",
        "if not importlib.util.find_spec('model_compression_toolkit'):\n",
        "    !pip install -q model_compression_toolkit==2.3.0\n",
        "\n",
        "# Observe, we use tutorials assets from mct version 2.2.0\n",
        "!rm -rf temp_mct\n",
        "!git clone https://github.com/sony/model_optimization.git temp_mct && cd temp_mct && git checkout v2.2.0 && cd ..\n",
        "!mv temp_mct/tutorials . && rm -rf temp_mct\n",
        "sys.path.insert(0,\"tutorials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr4f0enaAJF_"
      },
      "source": [
        "# Keras NanoDet float model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzPpgZ_VAJF_"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "assert '2.15.' in tf.__version__, print(tf.__version__)\n",
        "assert '2.5.' in torch.__version__, print(torch.__version__)\n",
        "\n",
        "from keras.models import Model\n",
        "import model_compression_toolkit as mct\n",
        "from tutorials.mct_model_garden.models_keras.nanodet.nanodet_keras_model import nanodet_plus_m\n",
        "from tutorials.mct_model_garden.models_keras.utils.torch2keras_weights_translation import load_state_dict\n",
        "from tutorials.mct_model_garden.models_keras.nanodet.nanodet_keras_model import nanodet_box_decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz4i7aYmAJF_"
      },
      "outputs": [],
      "source": [
        "# Upload the trained custom model\n",
        "CUSTOM_WEIGHTS_FILE = dst_path  # The NanoDet model trained with PPE dataset\n",
        "CLASS_NAMES = [\n",
        "  'safety-equipment',\n",
        "  'person',\n",
        "  'goggles',\n",
        "  'helmet',\n",
        "  'no-goggles',\n",
        "  'no-helmet',\n",
        "  'no-vest',\n",
        "  'vest']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "DATASET_TRAIN = 'dataset/PPE_Detection_Using_CV.v3i.coco/train'\n",
        "ANNOT_TRAIN = 'dataset/PPE_Detection_Using_CV.v3i.coco/train/_annotations.coco.json'\n",
        "DATASET_VALID = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid'\n",
        "ANNOT_VALID = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid/_annotations.coco.json'\n",
        "DATASET_REPR = DATASET_VALID\n",
        "ANNOT_REPR = ANNOT_VALID\n",
        "\n",
        "QUANTIZED_MODEL = 'nanodet-quant-ppe.keras'\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "N_ITER = 20  # 1 for testing, otherwise 20\n",
        "\n",
        "assert Path(CUSTOM_WEIGHTS_FILE).exists()\n",
        "assert Path(DATASET_REPR).exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4hValJSAJF_"
      },
      "outputs": [],
      "source": [
        "def get_model(weights=CUSTOM_WEIGHTS_FILE, num_classes=NUM_CLASSES):\n",
        "    INPUT_RESOLUTION = 416\n",
        "    INPUT_SHAPE = (INPUT_RESOLUTION, INPUT_RESOLUTION, 3)\n",
        "    SCALE_FACTOR = 1.5\n",
        "    BOTTLENECK_RATIO = 0.5\n",
        "    FEATURE_CHANNELS = 128\n",
        "\n",
        "    pretrained_weights = torch.load(weights, map_location=torch.device('cpu'))['state_dict']\n",
        "    # Generate Nanodet base model\n",
        "    model = nanodet_plus_m(INPUT_SHAPE, SCALE_FACTOR, BOTTLENECK_RATIO, FEATURE_CHANNELS, num_classes)\n",
        "\n",
        "    # Set the pre-trained weights\n",
        "    load_state_dict(model, state_dict_torch=pretrained_weights)\n",
        "\n",
        "    # Add Nanodet Box decoding layer (decode the model outputs to bounding box coordinates)\n",
        "    scores, boxes = nanodet_box_decoding(model.output, res=INPUT_RESOLUTION, num_classes=num_classes)\n",
        "\n",
        "    # Add TensorFlow NMS layer\n",
        "    outputs = tf.image.combined_non_max_suppression(\n",
        "        boxes,\n",
        "        scores,\n",
        "        max_output_size_per_class=300,\n",
        "        max_total_size=300,\n",
        "        iou_threshold=0.65,\n",
        "        score_threshold=0.001,\n",
        "        pad_per_class=False,\n",
        "        clip_boxes=False\n",
        "        )\n",
        "\n",
        "    model = Model(model.input, outputs, name='Nanodet_plus_m_1.5x_416')\n",
        "\n",
        "    print('Model is ready for evaluation')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwyTUX4QAJF_"
      },
      "outputs": [],
      "source": [
        "# known warning:  WARNING: head.distribution_project.project not assigned to keras model !!!\n",
        "float_model = get_model(CUSTOM_WEIGHTS_FILE, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rghNXuoqAJF_"
      },
      "source": [
        "# PTQ quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxhyU9cMAJF_"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Iterator, Tuple, List\n",
        "\n",
        "import cv2\n",
        "from tutorials.mct_model_garden.evaluation_metrics.coco_evaluation import coco_dataset_generator, CocoEval\n",
        "from edgemdt_tpc import get_target_platform_capabilities\n",
        "\n",
        "def nanodet_preprocess(x):\n",
        "    img_mean = [103.53, 116.28, 123.675]\n",
        "    img_std = [57.375, 57.12, 58.395]\n",
        "    x = cv2.resize(x, (416, 416))\n",
        "    x = (x - img_mean) / img_std\n",
        "    return x\n",
        "\n",
        "def get_representative_dataset(n_iter: int, dataset_loader: Iterator[Tuple]):\n",
        "    def representative_dataset() -> Iterator[List]:\n",
        "        ds_iter = iter(dataset_loader)\n",
        "        for _ in range(n_iter):\n",
        "            yield [next(ds_iter)[0]]\n",
        "\n",
        "    return representative_dataset\n",
        "\n",
        "def quantization(float_model, dataset, annot, n_iter=N_ITER):\n",
        "    # Load representative dataset\n",
        "    representative_dataset = coco_dataset_generator(dataset_folder=dataset,\n",
        "                                                    annotation_file=annot,\n",
        "                                                    preprocess=nanodet_preprocess,\n",
        "                                                    batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Get a TPC object representing the imx500 hardware\n",
        "    tpc = get_target_platform_capabilities(tpc_version='4.0', device_type='imx500')\n",
        "\n",
        "    # Preform post training quantization\n",
        "    quant_model, _ = mct.ptq.keras_post_training_quantization(\n",
        "        float_model,\n",
        "        representative_data_gen=get_representative_dataset(n_iter, representative_dataset),\n",
        "        target_platform_capabilities=tpc)\n",
        "\n",
        "    print('Quantized model is ready')\n",
        "    return quant_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7piMYdUSAJGA"
      },
      "outputs": [],
      "source": [
        "quant_model = quantization(float_model, DATASET_REPR, ANNOT_REPR)\n",
        "print(f'Representative dataset: {DATASET_REPR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGlt53tEAJGA"
      },
      "outputs": [],
      "source": [
        "# Observe that loading quantized model might require specification of custom layers,\n",
        "# see https://github.com/sony/model_optimization/issues/1104\n",
        "mct.exporter.keras_export_model(model=quant_model, save_model_path=QUANTIZED_MODEL)\n",
        "print(f'Quantized model saved: {QUANTIZED_MODEL}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HejVP-iAJGA"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from tutorials.mct_model_garden.evaluation_metrics.coco_evaluation import CocoDataset, model_predict, load_and_preprocess_image\n",
        "from tutorials.mct_model_garden.models_pytorch.yolov8.yolov8_postprocess import clip_boxes, clip_coords, scale_coords, scale_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K3c9Td4AJGA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Helper function to perform coco evaluation with custom dataset\n",
        "\"\"\"\n",
        "from typing import List, Dict, Tuple, Callable\n",
        "import numpy as np\n",
        "\n",
        "# slow no batch version\n",
        "def format_results(outputs: List, img_ids: List, orig_img_dims: List, output_resize: Dict, custom_labels: Callable) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Format model outputs into a list of detection dictionaries.\n",
        "\n",
        "    Args:\n",
        "        outputs (list): List of model outputs, typically containing bounding boxes, scores, and labels.\n",
        "        img_ids (list): List of image IDs corresponding to each output.\n",
        "        orig_img_dims (list): List of tuples representing the original image dimensions (h, w) for each output.\n",
        "        output_resize (Dict): Contains the resize information to map between the model's\n",
        "                 output and the original image dimensions.\n",
        "        custom_labels (Callable): A function to map label outputs. Typically, COCO re-map from 80 (model) to 91 (dataset)\n",
        "\n",
        "    Returns:\n",
        "        list: A list of detection dictionaries, each containing information about the detected object.\n",
        "    \"\"\"\n",
        "    detections = []\n",
        "    h_model, w_model = output_resize['shape']\n",
        "    preserve_aspect_ratio = output_resize['aspect_ratio_preservation']\n",
        "\n",
        "    image_id = img_ids\n",
        "    scores = outputs[1].numpy().squeeze()  # Extract scores\n",
        "    labels = (custom_labels(outputs[2].numpy())).squeeze()  # Provide a function to map label outputs\n",
        "    boxes = outputs[0].numpy().squeeze()  # Extract bounding boxes\n",
        "    boxes = scale_boxes(boxes, orig_img_dims[0], orig_img_dims[1], h_model, w_model, preserve_aspect_ratio)\n",
        "    for score, label, box in zip(scores, labels, boxes):\n",
        "        if score == 0.0:\n",
        "            continue\n",
        "        detection = {\n",
        "            \"image_id\": image_id,\n",
        "            \"category_id\": label,\n",
        "            \"bbox\": [box[1], box[0], box[3] - box[1], box[2] - box[0]],\n",
        "            \"score\": score\n",
        "        }\n",
        "        detections.append(detection)\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMs0XuiNAJGA"
      },
      "outputs": [],
      "source": [
        "custom_dataset = CocoDataset(dataset_folder=DATASET_VALID,\n",
        "                             annotation_file=ANNOT_VALID,\n",
        "                             preprocess=nanodet_preprocess)\n",
        "\n",
        "MODEL = float_model\n",
        "INPUT_RESOLUTION = 416\n",
        "\n",
        "output_resize = {'shape': (INPUT_RESOLUTION, INPUT_RESOLUTION), 'aspect_ratio_preservation': False}\n",
        "coco_predictions = []\n",
        "for idx, (im, anns) in enumerate(custom_dataset):\n",
        "    #if idx > 2:\n",
        "    #    continue\n",
        "    outputs = model_predict(MODEL, np.expand_dims(im, axis=0))  # 4 tensors: bbox, scores, classes, detections\n",
        "    detections = format_results(outputs, anns[0]['image_id'], anns[0]['orig_img_dims'], output_resize, lambda x: x)\n",
        "    coco_predictions.extend(detections)\n",
        "    if (idx + 1) % 50 == 0:\n",
        "        print(f'processed {(idx + 1)} images')\n",
        "\n",
        "print(len(coco_predictions))\n",
        "\n",
        "cocoGt=COCO(ANNOT_VALID)\n",
        "cocoDt=cocoGt.loadRes(coco_predictions)\n",
        "cocoEval = COCOeval(cocoGt,cocoDt,'bbox')\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()\n",
        "\"\"\"\n",
        "epochs = 20\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250\n",
        " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.507\n",
        " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.214\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.147\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.304\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.242\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.439\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.228\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.370\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.560\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhpaOiQ_AJGA"
      },
      "outputs": [],
      "source": [
        "custom_dataset = CocoDataset(dataset_folder=DATASET_VALID,\n",
        "                             annotation_file=ANNOT_VALID,\n",
        "                             preprocess=nanodet_preprocess)\n",
        "\n",
        "MODEL = quant_model\n",
        "INPUT_RESOLUTION = 416\n",
        "\n",
        "output_resize = {'shape': (INPUT_RESOLUTION, INPUT_RESOLUTION), 'aspect_ratio_preservation': False}\n",
        "coco_predictions = []\n",
        "for idx, (im, anns) in enumerate(custom_dataset):\n",
        "    #if idx > 2:\n",
        "    #    continue\n",
        "    outputs = model_predict(MODEL, np.expand_dims(im, axis=0))  # 4 tensors: bbox, scores, classes, detections\n",
        "    detections = format_results(outputs, anns[0]['image_id'], anns[0]['orig_img_dims'], output_resize, lambda x: x)\n",
        "    coco_predictions.extend(detections)\n",
        "    if (idx + 1) % 50 == 0:\n",
        "        print(f'processed {(idx + 1)} images')\n",
        "\n",
        "print(len(coco_predictions))\n",
        "\n",
        "cocoGt=COCO(ANNOT_VALID)\n",
        "cocoDt=cocoGt.loadRes(coco_predictions)\n",
        "cocoEval = COCOeval(cocoGt,cocoDt,'bbox')\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()\n",
        "\"\"\"\n",
        "epochs = 20\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.242\n",
        " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.495\n",
        " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.206\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.071\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.138\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.295\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.237\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.427\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.238\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.361\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "604hVU0_AJGB"
      },
      "source": [
        "# Visualize detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dybcxnixAJGB"
      },
      "outputs": [],
      "source": [
        "# Helper functions for visualization\n",
        "import numpy as np\n",
        "\n",
        "# draw a single bounding box onto a numpy array image\n",
        "def draw_bounding_box(img, annotation, scale, class_id, score):\n",
        "    row = scale[0]\n",
        "    col = scale[1]\n",
        "    x_min, y_min = int(annotation[1]*col), int(annotation[0]*row)\n",
        "    x_max, y_max = int(annotation[3]*col), int(annotation[2]*row)\n",
        "\n",
        "    color = (0,255,0)\n",
        "\n",
        "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n",
        "    text = f'{int(class_id)}: {score:.2f}'\n",
        "    cv2.putText(img, text, (x_min + 10, y_min + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "# draw all annotation bounding boxes on an image\n",
        "def annotate_image(img, output, scale, threshold=0.55):\n",
        "    b = output[0].numpy()[0]\n",
        "    s = output[1].numpy()[0]\n",
        "    c = output[2].numpy()[0]\n",
        "    for index, row in enumerate(b):\n",
        "        if s[index] > threshold:\n",
        "            #print(f'row: {row}')\n",
        "            id = int(c[index])\n",
        "            draw_bounding_box(img, row, scale, id, s[index])\n",
        "            print(f'class: {CLASS_NAMES[id]} ({id}), score: {s[index]:.2f}')\n",
        "    return {'bbox':b, 'score':s, 'classes':c}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbAefnpIAJGB"
      },
      "outputs": [],
      "source": [
        "# See appendix for results. For 2 epochs, the bounding boxes are not perfect...\n",
        "# But improves considerably for 20 epochs.\n",
        "\n",
        "MODEL = quant_model\n",
        "\n",
        "test_img = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid/image_257_jpg.rf.1a3a6eb456134cce302712c109645c26.jpg'\n",
        "img = load_and_preprocess_image(f'{test_img}', nanodet_preprocess)\n",
        "output = MODEL(np.expand_dims(img, axis=0))\n",
        "image = cv2.imread(f'{test_img}')\n",
        "print(f'image shape: {image.shape}')\n",
        "r = annotate_image(image, output, scale=image.shape)\n",
        "assert r['score'][0] > 0.5, print(f\"r['score'][0] > 0.5 failed: {r['score'][0]}\")\n",
        "dst = f'annotated.jpg'\n",
        "if cv2.imwrite(dst, image):\n",
        "    print(f'Annotated image saved to: {dst}')\n",
        "else:\n",
        "    print(f'Failed saving annotated image')\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq4ZV2UnAJGB"
      },
      "source": [
        "# Conversion\n",
        "For details see\n",
        "* [Raspberry Pi Documentation](https://www.raspberrypi.com/documentation/accessories/ai-camera.html#conversion)\n",
        "* [Sony IMX500 Converter documentation](https://developer.aitrios.sony-semicon.com/en/raspberrypi-ai-camera/documentation/imx500-converter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHDsFFC4AJGI"
      },
      "outputs": [],
      "source": [
        "!imxconv-tf -i {QUANTIZED_MODEL} -o converted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yORo-BpsAJGJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Expected output from converter:\n",
        "dnnParams.xml\t\t nanodet-quant-ppe_MemoryReport.json\n",
        "nanodet-quant-ppe.pbtxt  packerOut.zip\n",
        "\"\"\"\n",
        "!ls converted\n",
        "assert os.path.exists(\"converted/packerOut.zip\"), f\"Converted file not found\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V60jL6NAJGJ"
      },
      "source": [
        "# Next step\n",
        "__OBSERVE__: First, save the quantized model and the output from the conversion to your local machine. For packaging you will need the `packerOut.zip` file.\n",
        "\n",
        "Next step is to package the model for IMX500, see [Raspberry Pi Documentation](https://www.raspberrypi.com/documentation/accessories/ai-camera.html#packaging)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}